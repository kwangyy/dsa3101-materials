{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.\tAccuracy-Based Recommendations for Entities & Relationships\n",
    "o\tEntity-Level Recommendations: Leverage the NER vs. Ontology accuracy scores per entity type to prioritize areas for user review:\n",
    "\tLow-Accuracy Entities (<70%): Flag these as critical review areas for the user, as consistent errors here may indicate a need for additional training data or entity-class refinements.\n",
    "\tModerate Accuracy (70%-90%): Advise moderate review for entities where accuracy is acceptable but not ideal. Such cases may reveal areas needing slight ontology tweaks.\n",
    "\tHigh-Accuracy Entities (>90%): These may require occasional review but can generally be deprioritized, allowing the user to focus on other, lower-performing elements.\n",
    "o\tRelationship-Level Recommendations: Similarly, recommend checks based on relationship accuracy thresholds:\n",
    "\tFrequent Error Relationships: Identify relationships with lower accuracy scores (e.g., “LOCATED_AT” for geographical entities) and prompt users to confirm these connections or adjust ontology parameters accordingly.\n",
    "\tStable Relationships: Mark consistently high-performing relationships (e.g., 90%+ accuracy) as low-priority, indicating reliable extractions that may not need immediate attention.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.\tControlled Inconsistency Management\n",
    "o\tControlled inconsistencies allow the system to introduce minor variations and test if the model correctly aligns them to the appropriate ontology entries. This tests robustness while ensuring resolution mechanisms.\n",
    "\tMinor Variants: Introduce or identify minor variations for common entities (e.g., “John Doe” vs. “J. Doe”) to check if the model maps them accurately.\n",
    "\tConflicting Relationships: For cases where entities link to conflicting relationships (like a person having dual roles or affiliations across sources), the engine can suggest using recency, source reliability, or explicit user confirmation as conflict-resolution mechanisms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.\tPrompt Engineering Practices for Enhanced Entity-Relationship Analysis\n",
    "o\tError-Resistant Prompts: Use prompts that guide the model to double-check low-confidence entity matches and relationships. For example, prompts could include instructions like “<Whichever entities> have low confidence, please check.”\n",
    "o\tSource Relevance Testing: If there are conflicting sources (e.g., meeting minutes vs. emails), prompts should guide the system to weigh sources by criteria such as recency, reliability, or user-defined parameters to prioritize which source’s relationships to maintain. If user-defined parameters are not present, recommendation engine should give relevant explicit prompts to the user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.\tKnowledge Graph (KG) Consistency Checks\n",
    "o\tKG Verification: Periodically ensure the KG reflects ontology updates, such as adding new entities or correcting entity classifications.\n",
    "o\tConflict Flagging: Automatically flag entities with multiple, conflicting relationships (e.g., one person listed as an “Employee” and “External Consultant” for the same company) and prompt the user to review or reconcile these conflicts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. User Feedback for Prompt Engineering Improvements\n",
    "This component allows the recommendation engine to receive and adapt based on user feedback regarding the prompts' effectiveness. By tracking which prompts led to correct entity or relationship resolutions (as confirmed by the user) versus those that didn't, the engine can suggest prompt improvements over time.\n",
    "o  Feedback Collection: After a prompt is shown to the user, allow the user to rate the helpfulness of the prompt (e.g., “Helpful,” “Needs Improvement,” or “Not Relevant”).\n",
    "o  Analysis of Feedback: Based on feedback, adjust the structure or wording of future prompts. For instance, if prompts for certain entities or relationships consistently receive low helpfulness ratings, consider modifying them to be more detailed or context-specific.\n",
    "o  Adaptation and Learning: Implement a mechanism to record user feedback and adjust prompt styles accordingly. If a user indicates that a particular prompt type consistently needs improvement, this could trigger more detailed prompts for that entity type in the future.\n",
    "o  Prompt Suggestions: Provide suggestions for refining prompts. For example, if an entity-related prompt receives \"Needs Improvement\" feedback, suggest additional context that might help (e.g., adding source reliability scores or recentness)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Potential algorithms and implementation methods to be used <numbering don't correspond to above one>\n",
    "1. \tK-means Clustering: This unsupervised learning algorithm is chosen to detect clusters or patterns among entities and relationships. Its flexibility and simplicity make it ideal for identifying clusters based on accuracy, highlighting priority areas for user review.\n",
    "o\tJustification: While other clustering algorithms (like DBSCAN) could handle noise better, K-means offers a straightforward and interpretable way to group entities or relationships based on accuracy scores, allowing for a quick understanding of which areas need attention.\n",
    "2.\tConflict Resolution Framework: By setting up decision trees or rule-based systems that prioritize recent or more reliable data sources, the engine can resolve conflicts. Decision trees are chosen for their transparency, allowing users to adjust rules easily if necessary.\n",
    "3.  Feedback Logging: Store user feedback for each prompt type and entity in a feedback database or log.\n",
    "4.  Analysis and Adaptation: Regularly analyze the feedback to identify trends. For example, if feedback shows that prompts for certain entities are often marked as “Not Relevant,” consider adjusting those prompts to be more specific.\n",
    "5.  Dynamic Prompt Updates: Adjust prompt templates based on feedback trends, enabling the system to generate progressively refined prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import List, Dict\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import numpy as np\n",
    "\n",
    "class RecommendationEngine:\n",
    "    def __init__(self, accuracy_scores: Dict[str, int]):\n",
    "        self.accuracy_scores = accuracy_scores\n",
    "        self.prompt_feedback = {}  # Store feedback for each prompt\n",
    "    \n",
    "    def cluster_entities_for_review(self) -> List[Dict[str, str]]:\n",
    "        \"\"\"Clusters entities based on accuracy scores to prioritize review areas, returns JSON-serializable recommendations.\"\"\"\n",
    "        scores = np.array(list(self.accuracy_scores.values())).reshape(-1, 1)\n",
    "        kmeans = KMeans(n_clusters=3, random_state=0).fit(scores)\n",
    "        labels = kmeans.labels_\n",
    "\n",
    "        recommendations = []\n",
    "        for i, entity in enumerate(self.accuracy_scores.keys()):\n",
    "            score = self.accuracy_scores[entity]\n",
    "            priority = \"Critical\" if labels[i] == 0 else \"Moderate\" if labels[i] == 1 else \"Low\"\n",
    "            recommendations.append({\n",
    "                \"entity\": entity,\n",
    "                \"score\": score,\n",
    "                \"priority\": f\"{priority} review recommended\"\n",
    "            })\n",
    "        return recommendations\n",
    "\n",
    "    def recommend_relationship_checks(self) -> List[Dict[str, str]]:\n",
    "        \"\"\"Recommends checks for relationship accuracy based on thresholds, returns JSON-serializable recommendations.\"\"\"\n",
    "        recommendations = []\n",
    "        for rel in relationships:\n",
    "            rel_score = random.randint(60, 95)  # Simulating accuracy for demonstration\n",
    "            priority = \"High\" if rel_score < 75 else \"Moderate\" if rel_score < 85 else \"Low\"\n",
    "            recommendations.append({\n",
    "                \"relationship\": rel,\n",
    "                \"accuracy\": rel_score,\n",
    "                \"priority\": f\"{priority} priority check recommended\"\n",
    "            })\n",
    "        return recommendations\n",
    "\n",
    "    def handle_controlled_inconsistency(self, entity_variants: List[str]) -> List[Dict[str, str]]:\n",
    "        \"\"\"Handles controlled inconsistencies by checking minor variations in entity naming.\"\"\"\n",
    "        return [{\"entity_variant\": variant, \"suggested_action\": \"Verify if correctly mapped to expected entity\"}\n",
    "                for variant in entity_variants]\n",
    "\n",
    "    def resolve_conflicts(self) -> Dict[str, str]:\n",
    "        \"\"\"Identifies and resolves conflicting relationships using decision tree for prioritization.\"\"\"\n",
    "        conflict_data = np.array([[1, 0], [0, 1], [1, 1], [0, 0]])\n",
    "        conflict_labels = np.array([1, 1, 0, 0])  # Label: 1 means \"keep\", 0 means \"discard\"\n",
    "\n",
    "        clf = DecisionTreeClassifier(random_state=0)\n",
    "        clf.fit(conflict_data, conflict_labels)\n",
    "        new_conflict_case = np.array([[1, 1]])\n",
    "        resolution = clf.predict(new_conflict_case)\n",
    "\n",
    "        return {\"resolution\": \"Keep recent/reliable data\" if resolution[0] == 1 else \"Discard conflicting data\"}\n",
    "\n",
    "    def prompt_error_resistant(self, entities: List[str]) -> List[Dict[str, str]]:\n",
    "        \"\"\"Generates error-resistant prompts for entity verification.\"\"\"\n",
    "        return [{\"entity\": entity, \"prompt\": f\"Check for low-confidence matches or errors in '{entity}'.\"}\n",
    "                for entity in entities]\n",
    "\n",
    "    def prompt_source_relevance(self, conflicting_sources: Dict[str, List[str]]) -> List[Dict[str, str]]:\n",
    "        \"\"\"Generates prompts for assessing source relevance based on user-defined or default criteria.\"\"\"\n",
    "        prompts = []\n",
    "        for entity, sources in conflicting_sources.items():\n",
    "            prompts.append({\n",
    "                \"entity\": entity,\n",
    "                \"prompt\": \"Assess sources by recency and reliability for relevance.\",\n",
    "                \"sources\": sources\n",
    "            })\n",
    "        return prompts\n",
    "\n",
    "    def kg_verification(self) -> Dict[str, str]:\n",
    "        \"\"\"Simulated Knowledge Graph verification for consistency.\"\"\"\n",
    "        return {\"status\": \"KG verification complete\", \"details\": \"No discrepancies found.\"}\n",
    "\n",
    "    def conflict_flagging(self) -> List[Dict[str, str]]:\n",
    "        \"\"\"Flags entities with conflicting relationships for user review.\"\"\"\n",
    "        conflicts = [\n",
    "            {\"entity\": \"John Doe\", \"conflicting_roles\": [\"Employee\", \"External Consultant\"]}\n",
    "        ]\n",
    "        return [{\"entity\": conflict['entity'], \"conflicts\": conflict['conflicting_roles'],\n",
    "                 \"suggested_action\": \"User review required to reconcile roles\"} for conflict in conflicts]\n",
    "\n",
    "    def collect_feedback(self, prompt_id: str, feedback: str):\n",
    "        \"\"\"Collects feedback on prompts to refine prompt engineering based on user feedback.\"\"\"\n",
    "        if prompt_id in self.prompt_feedback:\n",
    "            self.prompt_feedback[prompt_id].append(feedback)\n",
    "        else:\n",
    "            self.prompt_feedback[prompt_id] = [feedback]\n",
    "\n",
    "    def analyze_feedback(self) -> Dict[str, str]:\n",
    "        \"\"\"Analyzes feedback and suggests improvements based on feedback patterns.\"\"\"\n",
    "        improvement_suggestions = {}\n",
    "        for prompt_id, feedback_list in self.prompt_feedback.items():\n",
    "            helpful_count = feedback_list.count(\"Helpful\")\n",
    "            needs_improvement_count = feedback_list.count(\"Needs Improvement\")\n",
    "            not_relevant_count = feedback_list.count(\"Not Relevant\")\n",
    "\n",
    "            # Suggestion based on feedback distribution\n",
    "            if needs_improvement_count > helpful_count:\n",
    "                improvement_suggestions[prompt_id] = \"Consider adding more context to this prompt.\"\n",
    "            elif not_relevant_count > helpful_count:\n",
    "                improvement_suggestions[prompt_id] = \"Refine to be more specific or actionable.\"\n",
    "            else:\n",
    "                improvement_suggestions[prompt_id] = \"No changes needed; prompt is effective.\"\n",
    "                \n",
    "        return improvement_suggestions\n",
    "\n",
    "# Instantiate recommendation engine\n",
    "accuracy_scores = {\"Person\": 85, \"Organization\": 92, \"Role\": 78, \"Location\": 67, \"Product/Service\": 95}\n",
    "recc_engine = RecommendationEngine(accuracy_scores)\n",
    "\n",
    "# Example usage for API responses\n",
    "def get_entity_recommendations():\n",
    "    return recc_engine.cluster_entities_for_review()\n",
    "\n",
    "def get_relationship_recommendations():\n",
    "    return recc_engine.recommend_relationship_checks()\n",
    "\n",
    "def get_controlled_inconsistency_recommendations():\n",
    "    return recc_engine.handle_controlled_inconsistency([\"AWS Architect\", \"A.W.S Architect\", \"AWS Archtct\"])\n",
    "\n",
    "def get_conflict_resolution_recommendations():\n",
    "    return recc_engine.resolve_conflicts()\n",
    "\n",
    "def get_error_resistant_prompts():\n",
    "    return recc_engine.prompt_error_resistant([\"Person\", \"Location\"])\n",
    "\n",
    "def get_source_relevance_prompts():\n",
    "    return recc_engine.prompt_source_relevance({\"Person\": [\"meeting_minutes\", \"email_archive\"]})\n",
    "\n",
    "def get_kg_verification():\n",
    "    return recc_engine.kg_verification()\n",
    "\n",
    "def get_conflict_flagging():\n",
    "    return recc_engine.conflict_flagging()\n",
    "\n",
    "def submit_feedback(prompt_id: str, feedback: str):\n",
    "    recc_engine.collect_feedback(prompt_id, feedback)\n",
    "\n",
    "def get_feedback_analysis():\n",
    "    return recc_engine.analyze_feedback()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
